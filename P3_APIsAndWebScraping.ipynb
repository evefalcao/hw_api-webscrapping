{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and transforming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check which videos from Vimeo's staff picks are featured at the blog Motionographer.com.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "* Generate an APP\n",
    "* Vimeo's API: https://developer.vimeo.com/api/guides/start\n",
    "* Token: https://developer.vimeo.com/apps/168643#personal_access_tokens\n",
    "* API wrapper pip install PyVimeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine\n",
    "from pandas.io.json import json_normalize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import requests, json, multiprocessing, glob, datetime, re, math\n",
    "\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(current_page):    \n",
    "\n",
    "    headers = {\"Authorization\": \"Bearer 6d797fb7512534142b202cc24aaab742\"}\n",
    "    endpoint = f'https://api.vimeo.com/channels/staffpicks/videos?page={current_page}&per_page=100'\n",
    "    vimeo_page = requests.get(endpoint, headers=headers)\n",
    "    page_content = vimeo_page.json()\n",
    "    return page_content\n",
    "\n",
    "def return_data(num_page):\n",
    "    response = get_page(num_page)\n",
    "    page_data = pd.json_normalize(response['data'])\n",
    "    page_data.to_csv(f'./downloaded_pages/v_page_{num_page:0>4}.csv')\n",
    "    print(f'Page {num_page:0>4} saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page_response = get_page(1)\n",
    "total_pages = math.ceil(first_page_response['total'] / first_page_response['per_page'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First function written (before using map for multiprocessing) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pages():\n",
    "    '''\n",
    "    Takes current_page number. Returns dict.\n",
    "    Return = {\n",
    "        'paging': {\n",
    "            'next': next page's uri or none\n",
    "        }, \n",
    "        'data': [\n",
    "            {},\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    response = get_page(1)\n",
    "    response_df = pd.json_normalize(response['data'])\n",
    "    pages_info = response['paging']\n",
    "    total_pages = math.ceil(response['total'] / response['per_page'])\n",
    "\n",
    "    for i in tqdm( range ( 1, ( total_pages + 1 ) ) ) :\n",
    "        response = get_page(i)\n",
    "        page_data = pd.json_normalize(response['data'])\n",
    "        page_data.to_csv(f'./downloaded_pages/v_page_{i:0>4}.csv')\n",
    "        print(f'Page {i:0>4} saved in disk.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to filter\n",
    "cols = ['name', 'link', 'duration', 'release_time', 'content_rating', 'tags', \n",
    "'categories', 'stats.plays', 'user.name', 'user.link', 'user.gender', 'user.websites', \n",
    "'user.account', 'user.websites', 'user.location_details.formatted_address','user.short_bio','user.skills', 'user.available_for_hire', 'user.location_details.latitude',\n",
    "'user.location_details.longitude', 'user.location_details.city',\n",
    "'user.location_details.state', 'user.location_details.neighborhood', 'user.location_details.sub_locality',\n",
    "'user.location_details.state_iso_code', 'user.location_details.country',\n",
    "'user.location_details.country_iso_code', 'width', 'height']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page = pd.read_csv('./downloaded_pages/v_page_0001.csv', usecols=cols)\n",
    "# l = pd.json_normalize(first_page['tags'][1])\n",
    "# type(first_page['tags'][1])\n",
    "# first_page['tags'] = pd.to_numeric(first_page['tags'], errors='ignore')\n",
    "# first_page['tags'][1]\n",
    "# json.load(first_page['tags'])\n",
    "json.loads(element for element in first_page['tags'])\n",
    "# json.loads(var)\n",
    "# first_page.dtypes\n",
    "# usecols=cols\n",
    "# 'tags', 'categories', 'user.websites', 'user.skills'\n",
    "\n",
    "# release_time\n",
    "# first_page.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each .csv has 100 rows, corresponding to 100 videos, and 175 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(first_page[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sending Parallel Requests to Vimeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Saves pages in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool()\n",
    "result = pool.map(return_data, range(1, total_pages + 1))\n",
    "pool.terminate()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Challenge: I begun the process by getting 25 videos per page, without multiprocessing and it was taking a whole night to download the pages, and either the kernel broke or I got some error in the middle of the process. Waiting for data was the most time consuming task in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering columns and merging all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './downloaded_pages/'\n",
    "all_files = glob.glob(path + \"*.csv\")\n",
    "each_csv = (pd.read_csv(f)[cols] for f in all_files)\n",
    "sp_df = pd.concat(each_csv, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting filtered .csv\n",
    "date_time = datetime.datetime.now().strftime(\"%d%b%Y\").replace('/', '').lower() \n",
    "sp_df.to_csv(f'./downloaded_pages/staffpicks_{date_time}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vimeo's Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_export = sp_df.sort_values(by='release_time', ascending=False)\n",
    "to_export.to_csv(f'./downloaded_pages/sp_{date_time}_tableau.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When I wrote the function to get all pages I forgot to add '.csv' when I named. I tried all possible methods to concat the files and got several errors. To sum it up, I was trying to concat .txt files, so when I imported the merged file, I was getting a very strange renderization (the file wasn't separated by comma, it was plai text!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Motionographer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Motionographer - curated motion design content: http://motionographer.com/ | https://motionographer.com/wp-json/wp/v2/posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first page\n",
    "first_page = 1\n",
    "first_page_link = f'http://motionographer.com/articles/page/{first_page}'\n",
    "m_soup = BeautifulSoup(requests.get('http://motionographer.com/articles/page/1').content)\n",
    "\n",
    "last_page_num = int(m_soup.select('body div nav li a')[-2].text)\n",
    "last_page_link = m_soup.select('body div nav li a')[-2]['href']\n",
    "all_pages_nums = [*range(first_page, last_page_num + 1)]\n",
    "all_pages_url = [f\"http://motionographer.com/articles/page/{item}\" for item in range(first_page, last_page_num + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(page_url):\n",
    "    '''Page url --> html. From page url, downloads html page.'''\n",
    "    \n",
    "    downloaded_page = BeautifulSoup(requests.get(page_url).content)\n",
    "    naming = re.findall('[0-9]+', str(page_url))\n",
    "    \n",
    "    with open(f\"./motionographer/m_{naming[0]}.html\", \"w\") as file:\n",
    "        file.write(str(downloaded_page))\n",
    "        \n",
    "#     print('page downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool()\n",
    "result = pool.map(download_page, all_pages)\n",
    "pool.terminate()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_post(post_url):\n",
    "    '''Post url --> html. From page url, downloads html page.'''\n",
    "    try:\n",
    "        downloaded_post = BeautifulSoup(requests.get(post_url).content)\n",
    "        naming = (re.findall('/([^/]+)/$', str(post_url)))[0]\n",
    "        post_df = crawl_posts(downloaded_post)\n",
    "        post_df.to_csv(f'./posts/{naming}.csv')\n",
    "    except IndexError:\n",
    "        print(f\"Couldn't download {post_url}\")\n",
    "        \n",
    "#     print('post downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_post_url(page_content):\n",
    "    '''Page content --> posts url. From soup, gets posts url.'''\n",
    "    \n",
    "    page_posts = BeautifulSoup(page_content).select('article.post > div.article-header > a')\n",
    "    url_list = [link['href'] for link in page_posts]\n",
    "#     print('page downloaded.')\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makes_soup(url):\n",
    "    '''Url --> soup. Makes soup from url.'''\n",
    "    \n",
    "    request = requests.get(url).content\n",
    "    soup = BeautifulSoup(request)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_posts(post_soup):\n",
    "    '''Post soup --> Pandas DataFrame. Returns a pandas dataframe from soup.'''\n",
    "    \n",
    "    title = post_soup.select('body article h1')[0].text # post title\n",
    "    try:\n",
    "        iframe = post_soup.select('div.video > iframe')[0]['src']\n",
    "        video_url = 'https://vimeo.com/' + ''.join(re.findall('/(\\d+)', iframe)) # vimeo links\n",
    "    except:\n",
    "        iframe = ''\n",
    "        video_url = ''\n",
    "    date = post_soup.select('body article p time')[0]['datetime'] # date / time\n",
    "    author = post_soup.select('body article p a')[0].text # author\n",
    "    author_url = post_soup.select('body article p a')[0]['href'] # author link\n",
    "    content = post_soup.select('body div article')[0] # article content\n",
    "    \n",
    "    post_page = {'Title': [title],'URL': [video_url], 'Date': [date], 'Author': [author], \n",
    "                 'Author_URL': [author_url], 'Content': [content]}\n",
    "    post_df = pd.DataFrame(post_page, index=[0])\n",
    "\n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html(page_num):\n",
    "    '''Page num --> String. Given a page number, opens and returns the page as a string.'''\n",
    "    \n",
    "    with open(f'./motionographer/m_{page_num}.html', 'r') as f:\n",
    "        html_string = f.read()\n",
    "    return html_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts_url(page_list):\n",
    "    '''Page --> posts in page. Gets list of pages and returns, for each page, a list with all posts.''' \n",
    "    \n",
    "    all_posts_url = []\n",
    "    \n",
    "    for page in page_list:\n",
    "        string = read_html(page)\n",
    "        posts_from_page = get_post_url(string)\n",
    "        all_posts_url.append(posts_from_page)\n",
    "        \n",
    "    return all_posts_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_posts_list = get_posts_url(all_pages_nums)\n",
    "flat_posts_list = [i for item in list_of_posts_list for i in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts paralel download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool()\n",
    "result = pool.map(download_post, tqdm(flat_posts_list))\n",
    "pool.terminate()\n",
    "pool.join()\n",
    "\n",
    "# folder = ('./posts/')\n",
    "# downloaded_posts = [download_post(x) for x in tqdm(flat_posts_list) if x not in folder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_posts = './posts/'\n",
    "all_files = glob.glob(path_posts + \"*.csv\")\n",
    "post_rows = (pd.read_csv(f) for f in tqdm(all_files))\n",
    "concated_file = pd.concat(post_rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = datetime.datetime.now().strftime(\"%d%b%Y\").replace('/', '').lower() \n",
    "concated_file.to_csv(f'./posts/motionographer_{date_time}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing data in a database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df = pd.read_csv('./downloaded_pages/sp_02apr2020_tableau.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vimeo_engine = create_engine('postgresql+psycopg2://postgres:123@localhost')\n",
    "# motionographer_engine = create_engine('postgresql+psycopg2://postgres:123@localhost/motionographer')\n",
    "engines = vimeo_engine\n",
    "conn = engines.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df.to_sql('staff_picks', conn, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Consolidate Pipeline\n",
    "5. Save Vimeo and Motionographer's data in a SQL database\n",
    "6. Update remote repo\n",
    "\n",
    "Extra:\n",
    "* Clean vimeo's data\n",
    "* Share on Kaggle\n",
    "* *Write content from it, with data visualization*\n",
    "* Share on LinkedIn with the community of designers/filmakers\n",
    "* Have 100% of functions with proper docstring description\n",
    "\n",
    "Done:\n",
    "1. Crawl all Motionographer pages\n",
    "2. Create dataset from it\n",
    "3. Filter useful information from Motionographer's posts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "343.4px",
    "left": "711.6px",
    "right": "20px",
    "top": "120px",
    "width": "292.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
